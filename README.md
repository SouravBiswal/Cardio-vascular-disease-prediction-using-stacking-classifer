# **HealthCoder 2023 - Cardiovascular Disease Prediction**

> A web app that predicts the probability of heart disease based on user input, offering a convenient and accessible tool for assessing individual risk.

![cvd home](https://github.com/somyasubham9/CVD_Prediction/assets/77459972/dcaea43a-ca64-4a5c-9062-8f2a23d49aa4)

## Abstract
Heart disease stands as a prominent global cause of death, highlighting the critical need for early detection to ensure effective treatment and prevention strategies. Recent advancements in machine learning (ML) techniques offer promising avenues for predicting heart disease risk. Nonetheless, the performance of individual ML models varies considerably, posing challenges in selecting the most suitable model for a specific dataset. In this study, we propose a heart disease prediction model employing a stacking ensemble approach that combines predictions of ten different ML models. 
This stacking ensemble is structured as a two-tier architecture. In the first layer, the ten base models generate predictions based on input features. These predictions are subsequently utilized as inputs for a meta-model, which is trained to determine the optimal combination of base model predictions. The primary objective of the meta-model is to efficiently weigh and combine predictions from the base models to maximize the overall predictive performance metric employed in the study.  


## Data
This dataset was created by combining different datasets already available independently but not combined before. In this dataset, 5 heart datasets are combined over 11 common features which makes it the largest heart disease dataset available so far for research purposes. The five datasets used for its curation are:

- Cleveland: 303 observations
- Hungarian: 294 observations
- Switzerland: 123 observations
- Long Beach VA: 200 observations
- Stalog (Heart) Data Set: 270 observations


Total: 1190 observations <br>
Duplicated: 272 observations<br>
`Final dataset: 918 observations`

## Data Visualization
In this study we have used Pandas Profiling on the dataset to provide an in-depth understanding of the data. The library generates descriptive statistics, including measures of central tendency, dispersion, and quantiles, for each variable. It also identifies missing values, data types, and unique values in the dataset.
Moreover, Pandas Profiling generates interactive visualizations such as histograms, bar plots, scatter plots, and correlation matrices. These visualizations allow data scientists to explore the relationships between different variables and identify potential patterns or trends related to heart disease risk factors.
The visualizations produced by Pandas Profiling help in understanding the distribution of variables, identifying outliers, and identifying potential correlations between variables. These insights can inform feature selection, data preprocessing, and model development stages of the heart disease prediction project. <br>
Here's a snapshot of one of the visualization generated by pandas profiling <br>

<img width="1171" alt="Screenshot 2023-05-30 at 10 00 21 PM" src="https://github.com/SouravBiswal/Cardio-vascular-disease-prediction-using-stacking-classifer/assets/69891272/455bcb56-f736-41ba-b865-905a3bd049e3">

<br>

`Note : You can download the pandas profile report.html on your local machine to see the visualization yourself!`



## Data Pre-processing

- Encoding the categorical features
- Scaling was performed (`Oldpeak` was _normalized_ while `Age, MaxHR and Cholestrol` were _standardized_)
- Feature selection was performed (Chi <sup> 2 </sup> test for _categorical features_ and ANOVA test for _numerical features_)
- Splitting the dataset into train and test sets

## Model Training
We have trained 10 different classifiers (base-models) and employed Grid-SearchCV with a Cross-validation of `5` folds and scored based on `AUC ROC` for finding the most optimal hyperparameters for each classifier. <br>
Models trained in order along with optimal hyperparameters :-

```python
1. SVC(C = 10, gamma = 'auto', kernel = 'linear')
2. KNeighborsClassifier(metric = 'manhattan', n_neighbors = 15, weights = 'distance')
3. MLPClassifier(activation = 'relu', alpha = 0.0001, early_stopping = False,
                                     hidden_layer_sizes = (50, 30, 20, 30, 10),
                                     learning_rate_init = 0.0001,
                                     max_iter = 500,solver = 'adam')
                                     
4. RandomForestClassifier(n_estimators = 150, max_features = 'auto', max_depth = 4, criterion = 'entropy')
5. XGBClassifier(n_estimators = 250, subsample = 0.8, min_child_weight = 1, max_depth = 3, gamma = 1.5, learning_rate = 0.02)
6. CatBoostClassifier(n_estimators = 200, depth = 4, l2_leaf_reg = 1, learning_rate = 0.03, verbose = False)
7. AdaBoostClassifier(n_estimators = 300, learning_rate = 0.03, algorithm = 'SAMME.R')
8. DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_leaf=10,random_state=42)
9. GaussianNB(var_smoothing = 0.0015199110829529332)
10. LogisticRegression(C=0.08858667904100823, max_iter=7,solver='saga',verbose = 0)
```
Thereafter, the probablities values for each classes were fed as input to the meta-model
 ```python
 final_estimator = CatBoostClassifier(n_estimators = 300, depth = 6, l2_leaf_reg = 1, learning_rate = 0.03, verbose = False)
 model = StackingClassifier(estimators=level0, final_estimator, stack_method = 'predict_proba', passthrough = True)
 ```
 > The meta-model can perform ever better, we believe it can be tuned further!
 
## Model Performance
Model performance was evaluated using `AUC ROC` as the scoring metric and RepeatedStratifiedKFold Cross-validation scheme
`n_splits = 5` and `n_repeats = 3` was followed to ensure each fold of dataset has the same proportion of observations.

|      Model            |  AUC ROC (%)|
|-----------------------|-------------|
|        SVC            |    89.2     |
|    KNeighbors         |    93.4     |
|    MLPClassifier      |    92.1     |
|    RandomForest       |    93.2     |
|    XGBClassifier      |    92.7     |
|    CatBoostClassifier |    93.2     |
|    AdaBoostClassifier |    91.6     |
|    DecisionTree       |    88.3     |
|    GaussianNB         |    89.9     |
|    LogisticRegression |    88.3     |
|    Stacked Ensemble   |    93.7     |
 

## Technology Stack:

ML: sklearn, pandas, numpy, matplotlib

Client: React,CSS,React-Router

Server: Python

## Further Study
The stacked ensemble's final-estimator can be fined tuned to give even better results. We've the included the code for the same in the notebook.
However, `Note : it's computationally super expensive and time taken to complete the execution can go from several hours to several days depending on the model used as final estimator and the parameter grid`

## Demo
Video Link: [https://drive.google.com/file/d/1Oh3mtgwJ8AfMGnt0_T-KSfUpU3TQeUgY/view?usp=sharing](https://drive.google.com/file/d/1GSDVmU2twGHsXhHdZuW3rTi9z3uFhD7a/view)

## Setup
To run this project, install it locally using npm:
Clone the Project
```
git clone  https://github.com/SouravBiswal/Cardio-vascular-disease-prediction-using-stacking-classifer.git
npm i
```
Client Side
```
$ cd client
$ npm install
$ npm start
```
Server Side
```
$ npm start
```
